{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4be999e7",
   "metadata": {},
   "source": [
    "\n",
    "## üß† CNN for Fault Detection in Casting Products\n",
    "\n",
    "This notebook demonstrates how to implement and train a **Convolutional Neural Network (CNN)**  to detect **defective vs. non-defective casting products** using a local dataset located in `./casting_data/`.\n",
    "\n",
    "#### Learning Goals\n",
    "By the end of this notebook, you will understand:\n",
    "- How CNNs process and learn image features for classification.\n",
    "- How to prepare an image dataset using `torchvision`.\n",
    "- How to build, train, and evaluate a CNN in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3072f168",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4769ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd273454",
   "metadata": {},
   "source": [
    "\n",
    "### 2Ô∏è‚É£ üì¶ Dataset: Real-Life Industrial Dataset of Casting Product\n",
    "\n",
    "In this notebook, we use the **[Real-Life Industrial Dataset of Casting Product](https://www.kaggle.com/datasets/ravirajsinh45/real-life-industrial-dataset-of-casting-product)** from Kaggle.  \n",
    "It contains **grayscale images** of automotive casted components ‚Äî both **defective** and **non-defective (OK)** parts ‚Äî captured in real industrial settings.\n",
    "\n",
    "#### üè≠ Dataset Overview\n",
    "- **Domain:** Industrial quality inspection / defect detection  \n",
    "- **Goal:** Classify cast products as *defective* or *OK* based on visual appearance  \n",
    "- **Structure:**\n",
    "\n",
    "- Total images: ~7,000  \n",
    "- Image size: 300√ó300 (grayscale)\n",
    "\n",
    "\n",
    "#### üíæ How to Download from Kaggle\n",
    "To use this dataset in your environment, you must first **authenticate** with your Kaggle account.\n",
    "\n",
    "1. Go to your Kaggle account ‚Üí [https://www.kaggle.com/settings](https://www.kaggle.com/settings)  \n",
    "2. Scroll to **API** ‚Üí click **Create New API Token**  \n",
    " - This downloads a file named `kaggle.json` containing your credentials.\n",
    "3. Move `kaggle.json` to:\n",
    " - **Windows:** `C:\\\\Users\\\\<YourName>\\\\.kaggle\\\\kaggle.json`\n",
    " - **Linux/Mac:** `~/.kaggle/kaggle.json`\n",
    "4. Install the Kaggle CLI:\n",
    " ```bash\n",
    " pip install kaggle\n",
    " kaggle datasets download -d ravirajsinh45/real-life-industrial-dataset-of-casting-product -p ./casting_data\n",
    " ```\n",
    "\n",
    "In this notebook, we will read data from a Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b803fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Mount Google Drive and Verify Dataset Path\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define dataset path inside your Drive\n",
    "data_dir = '/content/drive/MyDrive/Academia/KFUPM/Courses/ISE518/Data/casting_data'\n",
    "\n",
    "# Verify the dataset exists\n",
    "if os.path.exists(data_dir):\n",
    "    print(f'‚úÖ Dataset found at: {data_dir}')\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f'‚ùå Dataset folder not found at {data_dir}.\\n'\n",
    "        'Please upload it to your Google Drive under:\\n'\n",
    "        'MyDrive/Academia/KFUPM/Courses/ISE518/Data/casting_data'\n",
    "    )\n",
    "\n",
    "# Define train/test subdirectories\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "print(f'Training data path: {train_dir}')\n",
    "print(f'Testing data path:  {test_dir}')\n",
    "\n",
    "# Optional: List folders for sanity check\n",
    "print(\"Train folders:\", os.listdir(train_dir))\n",
    "print(\"Test folders:\", os.listdir(test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199978de",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Data Transformations and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6633, Testing samples: 715\n",
      "Classes: ['def_front', 'ok_front']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define transformations for training and validation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}, Testing samples: {len(test_data)}\")\n",
    "print(f\"Classes: {train_data.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a858736",
   "metadata": {},
   "source": [
    "### 4Ô∏è‚É£ Visualize Sample Images (OK vs Defective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6768a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically find one sample from each class\n",
    "ok_dir = os.path.join(train_dir, 'ok_front')\n",
    "def_dir = os.path.join(train_dir, 'def_front')\n",
    "\n",
    "ok_sample = os.path.join(ok_dir, os.listdir(ok_dir)[0])\n",
    "def_sample = os.path.join(def_dir, os.listdir(def_dir)[0])\n",
    "\n",
    "# Read images\n",
    "ok_img = mpimg.imread(ok_sample)\n",
    "def_img = mpimg.imread(def_sample)\n",
    "\n",
    "# Plot side-by-side comparison\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(ok_img, cmap='gray')\n",
    "plt.title(\"OK Casting\", weight='bold', size=18)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(def_img, cmap='gray')\n",
    "plt.title(\"Defective Casting\", weight='bold', size=18)\n",
    "plt.axis('off')\n",
    "\n",
    "# Save and show\n",
    "plt.savefig('casting_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Displayed one OK and one Defective casting image for visual comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a554f0",
   "metadata": {},
   "source": [
    "### 5Ô∏è‚É£ Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93973ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastingCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CastingCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 37 * 37, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = CastingCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Display CNN architecture summary for RGB images 150x150\n",
    "summary(model, input_size=(3, 150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25f6d7f",
   "metadata": {},
   "source": [
    "#### üß© Visualize Computational Graph (optional)\n",
    "\n",
    "‚úÖ You will also need to install pygraphviz by runing the following code in Anaconda terminal: `conda install -c conda-forge pygraphviz`\n",
    "\n",
    "‚úÖ Install pygraphviz in Google Colab\n",
    "\n",
    "1. Install Graphviz system libraries (including headers): `!apt-get install -y graphviz libgraphviz-dev`\n",
    "\n",
    "2. Install pygraphviz from PyPI, specifying the include and library paths: `!pip install pygraphviz --no-binary pygraphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "import torch\n",
    "\n",
    "# Create a dummy input: (batch_size=1, channels=3, height=150, width=150)\n",
    "dummy_input = torch.randn(1, 3, 150, 150).to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "y = model(dummy_input)\n",
    "\n",
    "# Generate and visualize the computational graph\n",
    "make_dot(y, params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e74c34",
   "metadata": {},
   "source": [
    "### 6Ô∏è‚É£ Define Loss, Optimizer, and Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 10\n",
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = (outputs.view(-1) > 0.5).float()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    epoch_acc = running_corrects.double() / len(train_data)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc.cpu())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, val_corrects = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.float().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1), labels)\n",
    "            preds = (outputs.view(-1) > 0.5).float()\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    val_losses.append(val_loss / len(test_data))\n",
    "    val_acc.append(val_corrects.double() / len(test_data))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}] Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc[-1]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9890c6",
   "metadata": {},
   "source": [
    "### 7Ô∏è‚É£ Visualize Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f109f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Val Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69376a2b",
   "metadata": {},
   "source": [
    "### 8Ô∏è‚É£ Evaluate and Test a Single Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f7097",
   "metadata": {},
   "source": [
    "#### üß© Function to Test a Single Image from a Chosen Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1390faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def test_single_image(category: str):\n",
    "    \"\"\"\n",
    "    Test one random image from a specified category ('def_front' or 'ok_front').\n",
    "\n",
    "    Args:\n",
    "        category (str): The test folder name ('def_front' or 'ok_front').\n",
    "\n",
    "    Behavior:\n",
    "        - Randomly picks one image from test_dir/category.\n",
    "        - Loads and preprocesses it.\n",
    "        - Runs model prediction.\n",
    "        - Displays the image with true and predicted labels.\n",
    "    \"\"\"\n",
    "    category = category.strip()\n",
    "    test_category_dir = os.path.join(test_dir, category)\n",
    "    \n",
    "    # Validate category\n",
    "    if not os.path.exists(test_category_dir):\n",
    "        raise ValueError(\"Invalid category! Choose either 'def_front' or 'ok_front'.\")\n",
    "    \n",
    "    # Pick random image\n",
    "    test_image_path = os.path.join(test_category_dir, random.choice(os.listdir(test_category_dir)))\n",
    "    \n",
    "    # Load image for visualization\n",
    "    img = mpimg.imread(test_image_path)\n",
    "    img_pil = Image.open(test_image_path).convert('RGB')\n",
    "    img_tensor = test_transforms(img_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor).item()\n",
    "    \n",
    "    # Interpret prediction based on dataset mapping {'def_front': 0, 'ok_front': 1}\n",
    "    label_pred = 'Defective' if output < 0.5 else 'OK'\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"True: {category} | Predicted: {label_pred} (Score: {output:.2f})\", weight='bold', size=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üßæ True Label: {category}\")\n",
    "    print(f\"üîÆ Predicted: {label_pred} (score: {output:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edda0b8",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b3663",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_single_image('def_front')\n",
    "# test_single_image('ok_front')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
